// ft_search_server.cpp
// Build: g++ -std=c++17 -O2 ft_search_server.cpp -pthread -o ft_search_server
// Usage: ./ft_search_server <index_file> [port]
// Example: ./ft_search_server myindex.bin 8080
//
// Simple HTTP JSON search API on top of ft_search engine.

#include <bits/stdc++.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <unistd.h>
#include <thread>
#include <regex>
#include <arpa/inet.h>
using namespace std;

// -------- Copy phần cấu trúc cần thiết từ ft_search.cpp (InvertedIndex + Searcher) --------
using DocId = uint32_t;
struct Posting { DocId doc; uint32_t tf; };

static inline string to_lower(string s){for(char&c:s)c=tolower(c);return s;}
static inline bool ends_with(const string&s,const string&suf){return s.size()>=suf.size()&&s.compare(s.size()-suf.size(),suf.size(),suf)==0;}
static inline vector<string> simple_split_tokens(const string &text){
    vector<string> out; string cur;
    for (char ch:text){ if(isalnum((unsigned char)ch)) cur.push_back((char)tolower(ch));
        else { if(!cur.empty()){out.push_back(cur);cur.clear();}}}
    if(!cur.empty())out.push_back(cur);
    return out;
}
static string tiny_stem(const string&w){
    string s=w; static const vector<string> suf={"ing","ed","ly","es","s","ment","ness","tion"};
    for(auto &sf:suf) if(s.size()>sf.size()+2&&ends_with(s,sf)){s.erase(s.size()-sf.size());break;}
    return s;
}
static const unordered_set<string> STOP={"the","and","is","in","at","of","a","to","it","for","on","as","by","an","this","that"};

struct InvertedIndex {
    unordered_map<string,vector<Posting>> inv;
    vector<string> doc_paths; vector<size_t> doc_lengths;
    size_t total_docs=0; double avg_doc_len=0.0;
    bool load_from(const string &file){
        ifstream f(file,ios::binary); if(!f)return false;
        f.read((char*)&total_docs,sizeof(total_docs)); f.read((char*)&avg_doc_len,sizeof(avg_doc_len));
        uint64_t nd; f.read((char*)&nd,sizeof(nd));
        doc_paths.resize(nd); doc_lengths.resize(nd);
        for(size_t i=0;i<nd;++i){
            uint64_t L; f.read((char*)&L,sizeof(L)); string p; p.resize(L); f.read(&p[0],L);
            uint64_t dl; f.read((char*)&dl,sizeof(dl)); doc_paths[i]=p; doc_lengths[i]=dl;
        }
        uint64_t termcount; f.read((char*)&termcount,sizeof(termcount));
        inv.clear(); inv.reserve(termcount*2);
        for(uint64_t i=0;i<termcount;++i){
            uint64_t tl; f.read((char*)&tl,sizeof(tl)); string term; term.resize(tl); f.read(&term[0],tl);
            uint64_t plist; f.read((char*)&plist,sizeof(plist));
            vector<Posting> posts(plist);
            for(uint64_t j=0;j<plist;++j){
                f.read((char*)&posts[j].doc,sizeof(DocId)); f.read((char*)&posts[j].tf,sizeof(uint32_t));
            }
            inv.emplace(move(term),move(posts));
        }
        total_docs=doc_paths.size(); return true;
    }
};

struct ScoredDoc { DocId doc; double score; };

class Searcher {
public:
    Searcher(InvertedIndex &idx):idx(idx){N=max<size_t>(1,idx.total_docs);}
    vector<ScoredDoc> search(const string&q,size_t topk=10){
        auto terms=parse_query(q); unordered_map<DocId,double> sc;
        for(auto&t:terms){auto it=idx.inv.find(t); if(it==idx.inv.end())continue;
            double idf=compute_idf(it->second.size());
            for(auto&p:it->second){
                double tf=p.tf; double len=idx.doc_lengths[p.doc];
                double s=idf*bm25_tf(tf,len); sc[p.doc]+=s;
            }
        }
        vector<ScoredDoc> res; res.reserve(sc.size());
        for(auto &kv:sc)res.push_back({kv.first,kv.second});
        sort(res.begin(),res.end(),[](auto&a,auto&b){return a.score>b.score;});
        if(res.size()>topk)res.resize(topk); return res;
    }
private:
    InvertedIndex &idx; size_t N; double k1=1.5,b=0.75;
    vector<string> parse_query(const string&q){
        auto toks=simple_split_tokens(q); vector<string> out;
        for(auto&t:toks){string s=tiny_stem(t); if(s.size()<2||STOP.count(s))continue; out.push_back(s);}
        return out;
    }
    double compute_idf(size_t df){return log(((double)N-df+0.5)/(df+0.5)+1.0);}
    double bm25_tf(double tf,double len){double norm=k1*((1-b)+b*(len/max(1.0,idx.avg_doc_len)));return tf*(k1+1)/(tf+norm);}
};

// -------- HTTP tiny server --------
static string url_decode(const string &src){
    string out; char a,b; for(size_t i=0;i<src.size();++i){
        if(src[i]=='%'&&i+2<src.size()&&isxdigit(src[i+1])&&isxdigit(src[i+2])){
            a=tolower(src[i+1]); b=tolower(src[i+2]);
            a=a>='a'?a-'a'+10:a-'0'; b=b>='a'?b-'a'+10:b-'0';
            out.push_back((char)(16*a+b)); i+=2;
        }else if(src[i]=='+') out.push_back(' ');
        else out.push_back(src[i]);
    } return out;
}

void handle_client(int client, InvertedIndex &idx){
    char buf[4096]; ssize_t r=read(client,buf,sizeof(buf)-1);
    if(r<=0){close(client);return;} buf[r]=0;
    string req(buf);
    smatch m;
    regex re("GET /search\\?q=([^ ]*) ");
    if(regex_search(req,m,re)){
        string q=url_decode(m[1]); Searcher s(idx);
        auto res=s.search(q,10);
        ostringstream body;
        body<<"{\"query\":\""<<q<<"\",\"results\":[";
        for(size_t i=0;i<res.size();++i){
            auto&r=res[i];
            if(i)body<<",";
            body<<"{\"doc\":"<<r.doc<<",\"score\":"<<r.score<<",\"path\":\""<<idx.doc_paths[r.doc]<<"\"}";
        }
        body<<"]}";
        string b=body.str();
        ostringstream head;
        head<<"HTTP/1.1 200 OK\r\nContent-Type: application/json\r\nContent-Length: "<<b.size()<<"\r\n\r\n";
        string resp=head.str()+b;
        write(client,resp.data(),resp.size());
    }else{
        string resp="HTTP/1.1 404 Not Found\r\nContent-Length:0\r\n\r\n";
        write(client,resp.data(),resp.size());
    }
    close(client);
}

int main(int argc,char**argv){
    if(argc<2){fprintf(stderr,"Usage: %s <index_file> [port]\n",argv[0]);return 1;}
    string idxfile=argv[1]; int port=8080; if(argc>=3)port=stoi(argv[2]);
    InvertedIndex idx; if(!idx.load_from(idxfile)){fprintf(stderr,"Cannot load index\n");return 2;}
    int srv=socket(AF_INET,SOCK_STREAM,0);
    int opt=1; setsockopt(srv,SOL_SOCKET,SO_REUSEADDR,&opt,sizeof(opt));
    sockaddr_in addr{}; addr.sin_family=AF_INET; addr.sin_addr.s_addr=INADDR_ANY; addr.sin_port=htons(port);
    if(bind(srv,(sockaddr*)&addr,sizeof(addr))<0){perror("bind");return 3;}
    listen(srv,16);
    printf("Server listening on port %d...\n",port);
    while(true){
        int cli=accept(srv,nullptr,nullptr);
        if(cli<0)continue;
        thread([cli,&idx]{handle_client(cli,idx);}).detach();
    }
}
// crawler.cpp
// Build: g++ -std=c++17 -O2 crawler.cpp -pthread -o crawler
// Run example:
//  ./crawler http://example.com 100 3 500 example.com
//  args: <seed_url> <max_pages> <workers> <politeness_ms> [allowed_domain]
//
// Features:
//  - multi-threaded BFS crawl up to max_pages
//  - per-host politeness (delay between requests to same host)
//  - respects robots.txt (User-agent: * simple Disallow rules)
//  - saves pages to pages/<n>.html and writes index.txt (URL -> file)
//  - basic normalization and deduplication of URLs
//
// Limitations:
//  - only HTTP (port 80), no HTTPS
//  - simple HTML link extraction via regex (good-enough for demo)
//  - robots parsing is basic (supports Disallow under User-agent: *)
//

#include <bits/stdc++.h>
#include <sys/socket.h>
#include <netdb.h>
#include <unistd.h>
#include <arpa/inet.h>
#include <fcntl.h>
#include <thread>
#include <mutex>
#include <condition_variable>
#include <atomic>
#include <chrono>
using namespace std;
using namespace std::chrono;

static const int HTTP_PORT = 80;
static const int CONNECT_TIMEOUT_MS = 5000;
static const int READ_TIMEOUT_MS = 5000;

// ---------- Utilities ----------
static inline string url_lower(const string &s) {
    string r = s;
    for (char &c : r) c = (char)tolower((unsigned char)c);
    return r;
}

static string host_from_url(const string &url) {
    // expects http://host[:port]/path
    size_t p = url.find("://");
    size_t start = (p==string::npos) ? 0 : p+3;
    size_t sl = url.find('/', start);
    string hostport = url.substr(start, sl==string::npos ? string::npos : sl-start);
    size_t colon = hostport.find(':');
    if (colon!=string::npos) return hostport.substr(0, colon);
    return hostport;
}

static string path_from_url(const string &url) {
    size_t p = url.find("://");
    size_t start = (p==string::npos) ? 0 : p+3;
    size_t sl = url.find('/', start);
    if (sl==string::npos) return "/";
    return url.substr(sl);
}

static bool is_absolute_url(const string &u) {
    return u.rfind("http://", 0) == 0 || u.rfind("https://", 0) == 0;
}

static string join_url(const string &base, const string &rel) {
    // base is absolute http://host/...; rel may be absolute or relative
    if (is_absolute_url(rel)) return rel;
    // ignore https relative -> we won't fetch https later but still produce URL
    // handle absolute path
    if (!rel.empty() && rel[0] == '/') {
        size_t p = base.find("://");
        size_t start = (p==string::npos) ? 0 : p+3;
        size_t sl = base.find('/', start);
        string host = (sl==string::npos) ? base : base.substr(0, sl);
        return host + rel;
    } else {
        // relative path: remove last segment of base path
        size_t p = base.find("://");
        size_t start = (p==string::npos) ? 0 : p+3;
        size_t sl = base.find('/', start);
        string prefix = (sl==string::npos) ? base + "/" : base.substr(0, sl+1);
        return prefix + rel;
    }
}

// simple URL normalization: lowercase scheme+host, remove fragment (#...), remove trailing slash (optional)
static string normalize_url(const string &url) {
    // minimal: lower scheme and host
    size_t p = url.find("://");
    if (p == string::npos) return url;
    string scheme = url_lower(url.substr(0, p));
    size_t start = p+3;
    size_t sl = url.find('/', start);
    string hostport = (sl==string::npos) ? url.substr(start) : url.substr(start, sl-start);
    string rest = (sl==string::npos) ? "/" : url.substr(sl);
    // remove fragment
    size_t hash = rest.find('#');
    if (hash != string::npos) rest = rest.substr(0, hash);
    // remove default port :80 if present
    if (hostport.size() > 3 && hostport.rfind(":80") == hostport.size()-3) {
        hostport = hostport.substr(0, hostport.size()-3);
    }
    // collapse ../ and ./ crudely - skip for simplicity
    // trim trailing slash except single "/"
    if (rest.size() > 1 && rest.back() == '/') rest.pop_back();
    return scheme + "://" + url_lower(hostport) + rest;
}

// Make filename safe
static string safe_filename(const string &s) {
    string out;
    for (char c : s) {
        if (isalnum((unsigned char)c) || c=='-' || c=='_' || c=='.') out.push_back(c);
        else out.push_back('_');
    }
    if (out.size() > 200) out = out.substr(0,200);
    return out;
}

// ---------- Simple HTTP client (blocking) ----------
// returns pair<status_code, body>. status_code = -1 on error
pair<int,string> http_get(const string &host, const string &path, int timeout_ms) {
    // resolve
    struct addrinfo hints{}, *res=nullptr;
    hints.ai_family = AF_UNSPEC;
    hints.ai_socktype = SOCK_STREAM;
    string port = to_string(HTTP_PORT);
    int rc = getaddrinfo(host.c_str(), port.c_str(), &hints, &res);
    if (rc != 0 || res == nullptr) return {-1, ""};

    int sock = -1;
    for (struct addrinfo *ai = res; ai; ai = ai->ai_next) {
        sock = socket(ai->ai_family, ai->ai_socktype, ai->ai_protocol);
        if (sock < 0) continue;
        // set non-blocking for connect with timeout
        int flags = fcntl(sock, F_GETFL, 0);
        fcntl(sock, F_SETFL, flags | O_NONBLOCK);
        int c = connect(sock, ai->ai_addr, ai->ai_addrlen);
        if (c < 0 && errno != EINPROGRESS) {
            close(sock);
            sock = -1;
            continue;
        }
        // wait for connect or timeout
        fd_set wf;
        FD_ZERO(&wf);
        FD_SET(sock, &wf);
        struct timeval tv;
        tv.tv_sec = timeout_ms / 1000;
        tv.tv_usec = (timeout_ms % 1000) * 1000;
        int sel = select(sock+1, nullptr, &wf, nullptr, &tv);
        if (sel <= 0) { close(sock); sock = -1; continue; }
        // check for errors
        int err = 0;
        socklen_t elen = sizeof(err);
        if (getsockopt(sock, SOL_SOCKET, SO_ERROR, &err, &elen) < 0 || err != 0) { close(sock); sock = -1; continue; }
        // set blocking
        fcntl(sock, F_SETFL, flags);
        break;
    }
    freeaddrinfo(res);
    if (sock < 0) return {-1, ""};

    // send request
    string req = "GET " + path + " HTTP/1.1\r\nHost: " + host + "\r\nUser-Agent: SimpleCrawler/1.0\r\nConnection: close\r\n\r\n";
    ssize_t s = send(sock, req.data(), req.size(), 0);
    if (s < 0) { close(sock); return {-1, ""}; }

    // read response with timeout
    string resp;
    char buf[4096];
    auto start = steady_clock::now();
    while (true) {
        // poll with timeout
        fd_set rf;
        FD_ZERO(&rf);
        FD_SET(sock, &rf);
        struct timeval tv;
        tv.tv_sec = READ_TIMEOUT_MS / 1000;
        tv.tv_usec = (READ_TIMEOUT_MS % 1000) * 1000;
        int sel = select(sock+1, &rf, nullptr, nullptr, &tv);
        if (sel <= 0) break;
        ssize_t r = recv(sock, buf, sizeof(buf), 0);
        if (r <= 0) break;
        resp.append(buf, buf + r);
        // safety: avoid huge responses
        if (resp.size() > 10 * 1024 * 1024) break;
        // optional timeout overall
        if (duration_cast<milliseconds>(steady_clock::now() - start).count() > timeout_ms + READ_TIMEOUT_MS) break;
    }
    close(sock);

    // parse status code and body
    size_t pos = resp.find("\r\n\r\n");
    if (pos == string::npos) return {-1, ""};
    string head = resp.substr(0, pos);
    string body = resp.substr(pos+4);
    // status code
    int status = -1;
    {
        istringstream iss(head);
        string httpver;
        iss >> httpver >> status;
        if (!iss) status = -1;
    }
    return {status, body};
}

// ---------- Robots.txt minimal parser ----------
struct RobotsRules {
    vector<string> disallow; // simple prefixes to block
};

static RobotsRules parse_robots_text(const string &text) {
    RobotsRules r;
    istringstream iss(text);
    string line;
    bool applies = false; // whether current User-agent section applies to us (we check *)
    while (getline(iss, line)) {
        // trim
        while (!line.empty() && isspace((unsigned char)line.back())) line.pop_back();
        size_t i = 0; while (i < line.size() && isspace((unsigned char)line[i])) ++i;
        if (i) line = line.substr(i);
        if (line.empty()) continue;
        // lowercase key
        string low = url_lower(line);
        if (low.rfind("user-agent:",0) == 0) {
            string val = low.substr(11);
            // trim
            while (!val.empty() && isspace((unsigned char)val[0])) val.erase(val.begin());
            applies = (val == "*" || val.find("simplecrawler") != string::npos);
        } else if (applies && low.rfind("disallow:",0) == 0) {
            string val = line.substr(9);
            while (!val.empty() && isspace((unsigned char)val[0])) val.erase(val.begin());
            if (val.empty()) continue;
            // ensure starts with '/'
            if (val[0] != '/') val = "/" + val;
            r.disallow.push_back(val);
        } else if (low.rfind("allow:",0) == 0) {
            // ignore allow for simplicity
        }
    }
    return r;
}

static bool robots_disallow_check(const RobotsRules &r, const string &path) {
    for (auto &p : r.disallow) {
        if (path.rfind(p, 0) == 0) return true;
    }
    return false;
}

// ---------- Link extraction (very simple) ----------
static vector<string> extract_links(const string &html) {
    vector<string> out;
    // find href="..."
    static const regex re(R"((?i)href\s*=\s*['"]([^'"]+)['"])");
    auto begin = sregex_iterator(html.begin(), html.end(), re);
    auto end = sregex_iterator();
    for (auto it = begin; it != end; ++it) {
        string u = (*it)[1].str();
        // clean spaces
        if (u.empty()) continue;
        // strip fragments
        size_t hash = u.find('#');
        if (hash != string::npos) u = u.substr(0, hash);
        if (u.empty()) continue;
        out.push_back(u);
    }
    return out;
}

// ---------- Crawler core ----------
struct FrontierItem { string url; int depth; };

class Crawler {
public:
    Crawler(const string &seed, int max_pages, int workers, int politeness_ms, const string &allowed_domain)
    : max_pages_(max_pages), politeness_ms_(politeness_ms), allowed_domain_(allowed_domain) {
        enqueue(seed, 0);
        workers_ = max(1, workers);
        mkdir_pages_dir();
    }

    void run() {
        vector<thread> pool;
        for (int i=0;i<workers_;++i) pool.emplace_back(&Crawler::worker_loop, this);
        // wait
        for (auto &t : pool) if (t.joinable()) t.join();
        // finalize index file
        write_index_file();
    }

private:
    // thread-safe structures
    deque<FrontierItem> frontier_;
    mutex frontier_mu_;
    condition_variable frontier_cv_;
    unordered_set<string> seen_; // normalized URL set
    mutex seen_mu_;

    // per-host robots & last access
    unordered_map<string, RobotsRules> robots_cache_;
    unordered_map<string, steady_clock::time_point> last_access_;
    mutex host_mu_;

    // saved pages mapping
    vector<pair<string,string>> saved_; // (url, filepath)
    mutex saved_mu_;

    atomic<int> pages_crawled_{0};

    int max_pages_;
    int workers_;
    int politeness_ms_;
    string allowed_domain_;

    void enqueue(const string &url, int depth) {
        string n = normalize_url(url);
        {
            lock_guard<mutex> lg(seen_mu_);
            if (seen_.count(n)) return;
            seen_.insert(n);
        }
        {
            lock_guard<mutex> lg(frontier_mu_);
            frontier_.push_back({n, depth});
        }
        frontier_cv_.notify_one();
    }

    bool pop_frontier(FrontierItem &it) {
        unique_lock<mutex> lk(frontier_mu_);
        frontier_cv_.wait(lk, [&]{ return !frontier_.empty() || pages_crawled_.load() >= max_pages_; });
        if (frontier_.empty()) return false;
        it = frontier_.front(); frontier_.pop_front();
        return true;
    }

    void worker_loop() {
        while (pages_crawled_.load() < max_pages_) {
            FrontierItem it;
            if (!pop_frontier(it)) break;
            // optional domain restriction
            if (!allowed_domain_.empty()) {
                string h = host_from_url(it.url);
                if (h.find(allowed_domain_) == string::npos) continue;
            }
            string host = host_from_url(it.url);
            string path = path_from_url(it.url);

            // robots check (fetch and cache)
            RobotsRules rules;
            {
                lock_guard<mutex> lg(host_mu_);
                if (robots_cache_.count(host)) rules = robots_cache_[host];
                else {
                    // fetch robots.txt
                    auto res = http_get(host, "/robots.txt", CONNECT_TIMEOUT_MS);
                    if (res.first == 200) {
                        rules = parse_robots_text(res.second);
                    } else {
                        rules = RobotsRules();
                    }
                    robots_cache_[host] = rules;
                }
            }
            if (robots_disallow_check(rules, path)) {
                // skip due to robots
                continue;
            }

            // politeness: ensure last access was > politeness_ms_
            {
                lock_guard<mutex> lg(host_mu_);
                auto now = steady_clock::now();
                if (last_access_.count(host)) {
                    auto last = last_access_[host];
                    auto diff = duration_cast<milliseconds>(now - last).count();
                    if (diff < politeness_ms_) {
                        // sleep for remaining
                        int waitms = politeness_ms_ - (int)diff;
                        this_thread::sleep_for(milliseconds(waitms));
                    }
                }
                last_access_[host] = steady_clock::now();
            }

            // perform HTTP GET
            pair<int,string> res = http_get(host, path, CONNECT_TIMEOUT_MS + READ_TIMEOUT_MS);
            if (res.first != 200) {
                // skip on error or non-200
                continue;
            }

            // save to file
            int idx = pages_crawled_.fetch_add(1);
            string fname = "pages/" + to_string(idx) + "_" + safe_filename(host) + ".html";
            {
                ofstream ofs(fname, ios::binary);
                if (ofs) ofs << res.second;
            }
            {
                lock_guard<mutex> lg(saved_mu_);
                saved_.push_back({it.url, fname});
            }

            // extract links and enqueue
            auto links = extract_links(res.second);
            for (auto &lnk : links) {
                string abs = join_url(it.url, lnk);
                // ignore mailto: and javascript:
                if (abs.rfind("mailto:",0) == 0) continue;
                if (abs.rfind("javascript:",0) == 0) continue;
                // normalize and enqueue
                string norm = normalize_url(abs);
                enqueue(norm, it.depth + 1);
            }
        }
    }

    void mkdir_pages_dir() {
        struct stat st;
        if (stat("pages", &st) != 0) {
#ifdef _WIN32
            _mkdir("pages");
#else
            mkdir("pages", 0755);
#endif
        }
    }

    void write_index_file() {
        ofstream ofs("index.txt");
        if (!ofs) return;
        lock_guard<mutex> lg(saved_mu_);
        for (auto &p : saved_) {
            ofs << p.first << "\t" << p.second << "\n";
        }
    }
};

// ---------- main ----------
int main(int argc, char** argv) {
    if (argc < 5) {
        fprintf(stderr, "Usage: %s <seed_url> <max_pages> <workers> <politeness_ms> [allowed_domain]\n", argv[0]);
        fprintf(stderr, "Example: %s http://example.com 100 4 500 example.com\n", argv[0]);
        return 1;
    }
    string seed = argv[1];
    int max_pages = stoi(argv[2]);
    int workers = stoi(argv[3]);
    int politeness_ms = stoi(argv[4]);
    string allowed_domain = "";
    if (argc >= 6) allowed_domain = argv[5];

    Crawler c(seed, max_pages, workers, politeness_ms, allowed_domain);
    c.run();
    printf("Crawl finished. index.txt written, pages saved to pages/ directory.\n");
    return 0;
}
